# Big O

Это очень важная концепция.

Big O – это язык и метрика, которые мы используем для описания эффективности алгоритмов.

Непонимание этого полностью может навредить вам при разработке алгоритма. Мало того, что вас можно осудить строго за то, что вы не понимаете Big O, но вы также будете пытаться оценить, когда ваш алгоритм становится быстрее или медленнее.

Освойте эту концепцию.

### Аналогия

Представьте себе следующий сценарий: у вас есть файл на жестком диске, и вам нужно отправить его своему другу, который живет по на другом конце страны. Вам нужно как можно быстрее доставить файл своему другу. Как его отправить? 

Первой мыслью большинства людей была электронная почта, FTP или некоторые другие способы пересылки через интернет. Эта мысль разума, но только наполовину правильна.

Если это небольшой файл, вы, безусловно, правы. Так как если доставлять его самостоятельно на самолете это может занять 5-10 часов, чтобы добраться до аэропорта, сесть на рейс, а затем долететь и доставить его вашему другу.

Но что, если файл действительно, действительно большой? Возможно ли, что быстрее физически доставить его с помощью самолета?

Да, на самом деле. Для передачи в электронном виде один терабайт (1 ТБ) может занять более одного дня. Было бы гораздо быстрее просто пролететь по всей стране. Если ваш файл является срочным (и стоимость не является проблемой), вы можете просто сделать это.

Что делать, если не было рейсов, а вместо этого вам нужно было ехать по стране на машине? Даже тогда, для действительно огромного файла, было бы быстрее прокатиться на машине.

### Time Complexity Оценка сложности выполнения временем

Big O можно представить в виде графика, где ось Y – время необходимое для выполнения алгоритма или задачи, а ось X – размер файла, как в примере с прошлой задачей или в общем представлении – размер объекта, над которым необходимо совершить работу.

Представим что размер файла 5 GB, и скорость передачи по интернету 1 GB в час.

Мы могли бы описать алгоритм «алгоритм передачи данных» как:

* Передача через интернет: O(5), ( читается как “О от пяти” ), где 5 - размер файла. Это означает, что время передачи файла увеличивается линейно с размером файла. (Да, это немного упрощение, но для этих целей это нормально). То есть если файл будет 7 GB, то сложность этого алгоритма будет O(7) – О от семи.

* Передача самолетом: 0(1), ( читается как “О от одного”) относительно размера файла. По мере увеличения размера файла времени больше не потребуется, чтобы передать файл для вашего друга. Время постоянное, так как можно за один раз перевести файл очень большого размера.

Независимо от того, насколько велика константа (в данном примере время пересылки файла, а именно O(5),  где 5 это константа)  и насколько медленным является линейное увеличение, линейная сложность будет в какой-то момент превосходить постоянную.

![Big O 1](img/big-o-1.png?raw=true)

А теперь давайте разберем предыдущий абзац. У нас есть 2 варианта пересылки: по интернету или самолетом. Вариант пересылки по интернету можно выразить как O(n). Вместо n подставляется реальное число, но когда нужно просто оценить алгоритм, то про реальное число не говорят, а просто: “О от n”. Значит, что время будет зависит от чего-то пропорционально. В данном примере от размера файла, так как скорость передачи файла постоянна.

В случае с самолетом сложность оценивается как O(1) “O от единицы”. Под единицей понимаем константное время за которое можно передать файл разного размера. 

Есть гораздо больше вариантов типов времени выполнения, чем это. Некоторые из наиболее распространенных из них - O(log N), O(N log N), O(N в степени 2) и O(2N). Тем не менее, нет фиксированного списка возможных вариантов времени выполнения.

Вы также можете иметь несколько переменных во время выполнения. Например, время рисования забора шириной w и шириной h метров может быть описано как O(wh). Если вам нужны r слоев краски, вы можете сказать, что время равно 0 (whr).

### Big O, Big Theta, and Big Omega 
Если вы никогда не занимались Big O в академических условиях, вы, вероятно, можете пропустить этот подраздел. Это может смутить вас больше, чем поможет. 

Этот раздел в основном, чтобы прояснить двусмысленность в формулировках для людей, которые понимают, что такое Big O.

Ученые используют большие O, большие Θ (тета) и большие Ω(омега) для описания времени автономной работы.

* O (Big O) В науке Big O описывает верхнюю границу времени. Алгоритм, который печатает все значения в массиве, может быть описан как O(N), но он также может быть описан как O(W), O(N3) или O(2N) (или много других O раз). Алгоритм не менее быстрый, чем каждый из них; поэтому они являются верхними границами во время выполнения. Это похоже на отношение «меньше или равно». 

Если Бобу исполнилось X лет (я предполагаю, что никто не живет в возрасте 130 лет), тогда вы могли бы сказать: X <=13. Также было бы правильно сказать, что X <= 1,000 или X <=1,000,000. Технические это истинное утверждение (хотя и не слишком полезно). Аналогично, простой алгоритм для отображения  значений массива равен O(N), а также O(N³) или любой среде выполнения больше O(N).

* Ω (большая омега): в науке Ω – эквивалентная концепция, но для нижней границы. Чтение значений из массива равна O(N), а также O(log N) и O(1). В конце концов, вы знаете, что это не будет быстрее, чем другие.

* Θ (большая тета): в науке Θ означает как Big O, так и Ω. То есть, алгоритм равен O(N), если он равен O(N) и Ω(N). Θ дает плотную оценку на во время выполнения.

В индустрии (и, следовательно, на интервью) люди, объединили Θ и O вместе. Индустриальное понимание Big O ближе к тому, что ученые называют Θ, поскольку было бы неправильно описывать итерацию по массиву как 0 (N²). Индустрия просто скажет, что это O(N).

Для этой книги мы будем использовать Big O так как индустрия имеет тенденцию использовать его: всегда старайтесь предлагать сжатое описание времени выполнения.

### Лучший случай, худший случай и ожидаемый случай
Мы можем фактически описать нашу среду выполнения для алгоритма тремя различными способами.

Давайте посмотрим на это с точки зрения быстрого сортировки. 

Быстрая сортировка выбирает случайный элемент как «точку перестановки», а затем меняет значения в массиве таким образом, что элементы, меньшие, чем точка перестановки, появляются перед элементами, большими, чем точка перестановки.

Это дает «частичную сортировку»: тогда он рекурсивно сортирует левую и правую стороны, используя аналогичный процесс. 

**Лучший случай**: если все элементы равны, то быстрая сортировка, в среднем, просто проходит через массив один раз. Это O(N). Это фактически немного зависит от реализации быстрой сортировки. Однако есть реализации, которые будут выполняться очень быстро на отсортированном массиве.

**Худший случай**: что делать, если нам действительно не повезло, а точка перестановки неоднократно является самым большим элементом в массиве? (На самом деле это легко осуществить. Если точка перестановки выбрана как первый элемент в части массива и массив отсортирован в обратном порядке, у нас будет такая ситуация.) В этом случае наша рекурсия не делит массив пополам и не рекурсирует на каждую половину. Он просто сжимает часть массива в один элемент. Это приведет к дегенерации времени работы O(N²).

**Ожидаемый случай**: Обычно эти чудесные или ужасные ситуации не случаются. Конечно, иногда точка перестановки будет очень низкой или очень высокой, но это не произойдет снова и снова. Мы можем ожидать время выполнения O(N log N). Мы редко когда-либо обсуждаем лучшую временную сложность, потому что это не очень полезная концепция. В конце концов, мы могли бы взять по существу любой алгоритм, специальный случай – некоторый ввод, а затем получить O(1) раз в лучшем случае.

Для многих, возможно, большинства алгоритмов, худший случай и ожидаемый случай одинаковы. Иногда они разные, и нам нужно описать оба времени работы.

*Какова связь между наилучшим / худшим / ожидаемым случаем и / Θ / Ω?*

Кандидатам легко путать эти понятия (возможно, потому, что у обоих есть некоторые понятия «выше»; «ниже» и «точно»), но между этими понятиями нет особой взаимосвязи. 

Наилучшие, худшие и ожидаемые случаи описывают большое время (или большую тета) для определенных ресурсов или сценариев. Big O, Big Omega и Big Theta описывают верхнюю, нижнюю и плотную границы для времени выполнения.

### Пространственная сложность
Время – это не единственное, что имеет значение в алгоритме. Мы также могли бы заботиться о количестве памяти, или дискового пространства, требуемого для работы алгоритма.

Пространственная сложность – это параллельная концепция временной сложности. Если нам нужно создать массив размером n, это ему будет нужно O(n) места.

Если нам нужен двухуровневый массив размером n*n, то в этом случае он будет занимать O(n²) места.

```java
int sum(int n) {/*Ex 1.*/
    if (n <= 0) {
        return 0;
    }
    return n + sum(n-1);
}
```
Каждый вызов добавляет уровень в стек.

```
sum(4)
    -> sum(3)
        -> sum(2)
            -> sum(l)
                -> sum(0)
```
Каждый из этих вызовов добавляется в стек вызова и занимает реальную память.

Однако то, что у вас всего n вызовов, не означает, что он занимает O(n) места. Рассмотрим функцию ниже, которая суммирует соседние элементы между O и n:

```java
int pairSumSequence(int n) {
    int sum = 0;
    for (int i = 0; i < n; i++) {
        sum += pairSum(i, i + 1);
    }
    return sum;
}

int pairSum(int a, int b) {
    return a + b;    
}
```
Будет примерно O(n) вызовов pairSum. Однако эти вызовы не существуют одновременно в стеке вызовов, поэтому вам нужно только O(1) место.

### Отбросьте константы. 
Код O(N) может работать быстрее, чем 0(1) кода для определенных входных данных. Big O просто описывает скорость роста. 

По этой причине мы отбрасываем константы во время выполнения. Алгоритм, который можно было бы описать как O(2N), на самом деле является O(N). Многие люди сопротивляются этому. Они увидят код с двумя (не вложенными) циклами и продолжат это значение 0(2N). Они думают, что они более "точны". Это не так.

Рассмотрим следующий код:
**Min and Max 1**
```java 
int min = Integer.MAX_VALUE; 
int max = Integer.MIN_VALUE; 
for (int x : array) {
    if (x < min) min x; 
    if (x > max) max = x;
}
```
**Min and Max 2**
```java
int min = Integer.MAX_VALUE;
int max = Integer.MIN_VALUE;
for (int x : array) {
    if (x < min) min = x;
}
for (int x : array) {
    if (x > max) maxn = x;
}
```

Какой из них быстрее? Первый делает один цикл, а другой делает два для цикла. Но тогда первое решение имеет две строки кода для цикла for, а не одну.

Если вы собираетесь подсчитать количество инструкций, вам нужно перейти на уровень сборки и принять во внимание, что для умножения требуется больше инструкций, чем для сложения, как компилятор что-то оптимизирует, и всякое такое.

Это может быть ужасно сложно, поэтому даже не начинайте идти по этому пути. Big O позволяет нам выразить, как масштабируется среда выполнения. Нам просто нужно признать, что это не значит, что O(N) всегда лучше, чем O (N²).

### Отбросьте недоминантные условия
Что вы делаете с таким выражением, как O(N² + N)? Этот вторая N не совсем константа. Но это не особенно важно.

Мы уже говорили, что отбрасываем константы. Поэтому O(N² + N²) будет O(N²). Если нас не волнует этот последний термин N2, зачем нам заботиться о N? И не будем.

Вы должны отказаться от недоминирующих условий.
* O(N² + N) становится O(N²).
* O(N + log N) становится O(N).
* O(5 * 2ⁿ + 1000N¹¹¹) становится O(2ⁿ).

У нас еще может быть сумма во время выполнения. Например, выражение O(B² + A) не может быть уменьшено (без
некоторые специальные знания об A и B).

На следующем графике показана скорость увеличения для некоторых распространенных больших значений O.

![Big O 1](img/big-o-2.png?raw=true)

Также вы можете видеть, что O(x²) намного больше, чем O(x), но это совсем не так плохо, как (2ˣ) или O(x!).
Также имеется множество случаев, аремя выполнения больше чем (x!), Например O(xˣ) или 0 (2ˣ * x!).

### Алгоритмы, состоящие из нескольких частей: сложение vs. умножение
Предположим, у вас есть алгоритм, который состоит из двух шагов. Когда вы должны умножать время выполнения и 
когда складывать?

Это общий источник путаницы для кандидатов.

**Сложение врмени выполнения O(A + B)**
```java
for(int a : arrA){
    print(a)
}

for(int b : arrB){
    print(b)
}
```
В этом примере мы выполняем работу A, а затем работу B. Следовательно, общее количество работ O(A + B).

**Умножение врмени выполнения O(A * B)**
```java
for(int a : arrA){
    for(int b : arrB){
        print(a)
        print(b)
    }
}
```
В этом справа мы выполняем работу B  для каждого элемента в A. Поэтому общий объем работы составляет O(A * B).

Другими словами:
* Если ваш алгоритм имеет форму «сделай это, то, когда все будет готово, сделай это», тогда вы складываете время выполнения.
* Если ваш алгоритм имеет форму «делайте это каждый раз, когда вы делаете это», то вы умножаете время выполнения.

Это очень легко напутать в интервью, поэтому будьте осторожны.

### Амортизированное время (Amortized Time)
`ArrayList`, или динамически изменяемый массив, позволяет использовать преимущества массива, предлагая гибкость в размере. 
Вам не хватит места в `Arraylist`, так как его емкость будет расти по мере вставки элементов.

`Arraylist` реализован с помощью массива. Когда массив достигнет емкости, класс `Arraylist` создаст новый массив с 
удвоенной емкостью и скопирует все элементы в новый массив.

Как вы описываете время выполнения вставки? Это сложный вопрос.

Массив может быть полным. Если массив содержит N элементов, то вставка нового элемента займет O(N) времени. 
Вам нужно будет создать новый массив размером 2N, а затем скопировать N элементов. Эта вставка займет O(N) времени.

Однако мы также знаем, что это случается не очень часто. Подавляющее большинство времени вставки будет в O(1) времени.
Нам нужна концепция, которая учитывает и то, и другое. Это то, что делает амортизированное время.
Это позволяет нам описать, что да, этот худший случай случается время от времени. 
Но как только это произойдет, это не повторится снова настолько долго, потому что издержки «амортизируются»:

В этом случае, что такое амортизированное время?

Когда мы вставляем элементы, мы удваиваем емкость массива, до тех пор пока размер массива не станет равен 
размеру массива возведенного в степень 2.
 
Поэтому после X элементов мы удваиваем емкость при размерах массива `1, 2, 4, 8, 16, ..., X`. 
Удвоение занимает соответственно `1, 2, 4, 8, 16, 32, 64, ...`, X копий.

Какова сумма `1 + 2 + 4 + 8 + 16 + ... + X`? Если вы читаете эту сумму слева направо, она начинается с 1 и удваивается до X. 
Если вы читаете справа налево, она начинается с X и делится пополам до 1.

Что такое сумма `X + X/2 + X/4 + X/8 + ... + 1`? Это примерно `2X`.

Следовательно, для X-вставок требуется O(2X) времени. Амортизированное время для каждой вставки составляет O(1).

### Log N Runtimes
Мы обычно видим O(log N) во времени выполнения. Откуда это берется?

Давайте посмотрим на бинарный поиск в качестве примера. 

В бинарном поиске мы ищем, например, `x` в отсортированном массиве. В этом массиве N-элементов. 

* Сначала мы сравним `х` с элементов в середине массива (назовем этот элемент `middle`). 
* Если `х == middle`, то это искомый результат и возвращаем его'. 
* Если `x < middle`, то мы ищем в левой стороне от `middle` элемента.
* Если `x > middle`, тогда мы ищем в правой стороне массива.

Превдокод этого алгоритма выглядит так:
```
search 9 within {1, 5, 8, 9, 11, 13, 15, 19, 21} 
    compare 9 to 11 -> smaller.
    search 9 within {1, 5, 8, 9, 11}
        compare 9 to 8 -> bigger
        search 9 within {9, 11}
            compare 9 to 9
             return
```

С комментариями
```
ищем 9 среди {1, 5, 8, 9, 11, 13, 15, 19, 21} 
    сравниваем 9 с 11 (11 именно потому что 11 это примерно середина массива) -> 11 меньше чем ищем, продложаем поиск в левой части 
    ищем 9 среди левой части {1, 5, 8, 9, 11} 
        сравниваем 9 с 8 -> больше чем ищем, продолжаем поиск в правой части
        ищем 9 среди {9, 11}
            равниваем 9 с 9
             возвращаем
```

* Мы начнем поиск в массиве, состоящего из N-элементов.
* Затем, мы переходим к элементу N/2, то есть в середиу массва.
* Еще один шаг, и мы на N/4 элементе.
* Мы останавливаемся, когда либо находим значение, либо когда остается тольео один элемент.
* Общее время выполнения зависит от того, сколько шагов (каждый раз делим N на 2) мы можем сделать 
до тех пор пока N не станет равно 1.

```
N = 16
N = 8    делим на 2 
N = 4    делим на 2
N = 2    делим на 2
N = 1    делим на 2
```

Мы могли бы взглянуть на это в обратном порядке (переходя от 1 к 16 вместо 16 к 1). 
Сколько раз мы можем умножить 1 на 2, пока не получим N?

```
N = 1    умножаем на 2
N = 2    умножаем на 2
N = 4    умножаем на 2
N = 8    умножаем на 2 
N = 16
```

Что такое `k` в выражении 2ᴷ = N? Это именно то, что выражает log (логарифм).

 ```
 2⁴ = 16 ⟶ log₂16 = 4
 log₂N = k ⟶ 2ᴷ = N
 ```

Это хороший вывод для вас. Когда вы видите проблему, где количество элементов в
проблемном пространстве сокращается вдвое, вероятно временем выполнения будет O(log N).

Это та же самая причина, по которой **поиск элемента в сбалансированном бинарном дереве составляет O(log N)**.
С каждым сравнением, мы идем влево или вправо. 
Половина узлов на каждой стороне, поэтому мы каждый раз сокращаем пространство проблемы пополам.
